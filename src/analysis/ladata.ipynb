{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17b463f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from statsmodels.genmod import families\n",
    "import matplotlib.pyplot as plt # For potential plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e2d4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r'../../data/processed/charity_main_cleaned.csv')\n",
    "birmingham_disposal = pd.read_excel(r'../../data/raw/birm_Disposal( in period 09052025).xlsx')\n",
    "#ladataset = pd.read_excel(r'../../data/raw/Local_government_finance_data_download.xlsx',sheet_name='Spending power totals')\n",
    "#housing_net_supply_df = pd.read_excel('../../data/raw/Local-authority-housing-supply.xlsx', sheet_name='Net supply')\n",
    "#housing_stock_df = pd.read_excel('../../data/raw/Local-authority-housing-supply.xlsx', sheet_name='Housing stock')\n",
    "#population = pd.read_excel('../../data/raw/population_by_age.xlsx', sheet_name='Age bands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "458b978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 114 notice dates by local authority (first relevant notice for each)\n",
    "s114_dates = {\n",
    "    'Nottingham': '2021-12-15',\n",
    "    'Birmingham': '2023-09-05',\n",
    "    'Woking': '2023-06-07',\n",
    "    'Thurrock': '2022-12-18',\n",
    "    'Croydon': '2020-11-11',\n",
    "    'Slough': '2020-11-11',\n",
    "    'North Northamptonshire': '2018-02-02',\n",
    "    'West Northamptonshire': '2018-02-02',\n",
    "}\n",
    "\n",
    "# Convert to datetime\n",
    "s114_dates = {k: pd.to_datetime(v) for k, v in s114_dates.items()}\n",
    "\n",
    "# Map treatment date based on local authority\n",
    "dataset['treatment_date'] = dataset['local_authority'].map(s114_dates)\n",
    "\n",
    "# Create binary treatment indicator\n",
    "dataset['treatment'] = dataset['treatment_date'].notna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9589a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of charities considered removed by 2025-03-31 00:00:00: 619\n",
      "Total Birmingham charities in scope: 3085\n"
     ]
    }
   ],
   "source": [
    "df_bham_charities = dataset[dataset['local_authority'] == 'Birmingham'].copy()\n",
    "s114_date = pd.to_datetime('2023-09-05') # Birmingham Section 114 notice date\n",
    "end_of_study_date = pd.to_datetime('2025-03-31') # Your actual end date\n",
    "df_bham_charities['date_of_registration'] = pd.to_datetime(df_bham_charities['date_of_registration'], errors='coerce')\n",
    "df_bham_charities['date_of_removal'] = pd.to_datetime(df_bham_charities['date_of_removal'], errors='coerce')\n",
    "\n",
    "df_bham_charities['is_removed_by_end_of_study'] = (\n",
    "    (df_bham_charities['date_of_removal'].notna()) &\n",
    "    (df_bham_charities['date_of_removal'] <= end_of_study_date)\n",
    ").astype(int)\n",
    "\n",
    "print(f\"Number of charities considered removed by {end_of_study_date}: {df_bham_charities['is_removed_by_end_of_study'].sum()}\")\n",
    "print(f\"Total Birmingham charities in scope: {len(df_bham_charities)}\")\n",
    "\n",
    "# Section 114 flag at end_of_study_date\n",
    "df_bham_charities['section_114_flag_at_end_of_study'] = int(s114_date <= end_of_study_date)\n",
    "\n",
    "# Cumulative disposals up to end_of_study_date\n",
    "birmingham_disposal['DISPLDATE'] = pd.to_datetime(birmingham_disposal['DISPLDATE'])\n",
    "\n",
    "# Filter disposals up to the end_of_study_date\n",
    "relevant_disposals_at_end = birmingham_disposal[\n",
    "    (birmingham_disposal['DISPLDATE'] <= end_of_study_date)\n",
    "].copy()\n",
    "\n",
    "\n",
    "# --- 6. Select final features for the model ---\n",
    "# Drop columns that were problematic (zero variance) or are no longer needed\n",
    "final_df_for_logistic = df_bham_charities.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cd1511e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(88033)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Other'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a526abde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 411 rows due to NaNs in model features or target.\n",
      "\n",
      "--- Data for Logistic Regression (First 5 rows) ---\n",
      "     section_114_flag_at_end_of_study charity_has_land  \\\n",
      "353                                 1             True   \n",
      "494                                 1            False   \n",
      "596                                 1            False   \n",
      "690                                 1             True   \n",
      "709                                 1            False   \n",
      "\n",
      "     is_removed_by_end_of_study  \n",
      "353                           0  \n",
      "494                           1  \n",
      "596                           0  \n",
      "690                           0  \n",
      "709                           0  \n",
      "Number of observations for Logistic Regression: 2674\n",
      "Number of 'removed' outcomes: 342\n",
      "Number of 'not removed' outcomes: 2332\n",
      "section_114_flag_at_end_of_study    int64\n",
      "charity_has_land                    int64\n",
      "dtype: object\n",
      "int64\n",
      "\n",
      "--- Logistic Regression Model Summary ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>is_removed_by_end_of_study</td> <th>  No. Observations:  </th>  <td>  2674</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                       <td>GLM</td>            <th>  Df Residuals:      </th>  <td>  2672</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>             <td>Binomial</td>          <th>  Df Model:          </th>  <td>     1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>              <td>Logit</td>           <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                     <td>IRLS</td>            <th>  Log-Likelihood:    </th> <td> -1011.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                 <td>Thu, 03 Jul 2025</td>      <th>  Deviance:          </th> <td>  2023.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                     <td>22:09:25</td>          <th>  Pearson chi2:      </th> <td>2.67e+03</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>               <td>5</td>             <th>  Pseudo R-squ. (CS):</th> <td>0.007988</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>          <td>nonrobust</td>         <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                  <td></td>                    <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>section_114_flag_at_end_of_study</th> <td>   -1.7511</td> <td>    0.066</td> <td>  -26.463</td> <td> 0.000</td> <td>   -1.881</td> <td>   -1.621</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>charity_has_land</th>                 <td>   -0.6143</td> <td>    0.138</td> <td>   -4.438</td> <td> 0.000</td> <td>   -0.886</td> <td>   -0.343</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                         & is\\_removed\\_by\\_end\\_of\\_study & \\textbf{  No. Observations:  } &     2674    \\\\\n",
       "\\textbf{Model:}                                 &               GLM               & \\textbf{  Df Residuals:      } &     2672    \\\\\n",
       "\\textbf{Model Family:}                          &             Binomial            & \\textbf{  Df Model:          } &        1    \\\\\n",
       "\\textbf{Link Function:}                         &              Logit              & \\textbf{  Scale:             } &    1.0000   \\\\\n",
       "\\textbf{Method:}                                &               IRLS              & \\textbf{  Log-Likelihood:    } &   -1011.7   \\\\\n",
       "\\textbf{Date:}                                  &         Thu, 03 Jul 2025        & \\textbf{  Deviance:          } &    2023.5   \\\\\n",
       "\\textbf{Time:}                                  &             22:09:25            & \\textbf{  Pearson chi2:      } &  2.67e+03   \\\\\n",
       "\\textbf{No. Iterations:}                        &                5                & \\textbf{  Pseudo R-squ. (CS):} &  0.007988   \\\\\n",
       "\\textbf{Covariance Type:}                       &            nonrobust            & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{section\\_114\\_flag\\_at\\_end\\_of\\_study} &      -1.7511  &        0.066     &   -26.463  &         0.000        &       -1.881    &       -1.621     \\\\\n",
       "\\textbf{charity\\_has\\_land}                     &      -0.6143  &        0.138     &    -4.438  &         0.000        &       -0.886    &       -0.343     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Generalized Linear Model Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                     Generalized Linear Model Regression Results                      \n",
       "======================================================================================\n",
       "Dep. Variable:     is_removed_by_end_of_study   No. Observations:                 2674\n",
       "Model:                                    GLM   Df Residuals:                     2672\n",
       "Model Family:                        Binomial   Df Model:                            1\n",
       "Link Function:                          Logit   Scale:                          1.0000\n",
       "Method:                                  IRLS   Log-Likelihood:                -1011.7\n",
       "Date:                        Thu, 03 Jul 2025   Deviance:                       2023.5\n",
       "Time:                                22:09:25   Pearson chi2:                 2.67e+03\n",
       "No. Iterations:                             5   Pseudo R-squ. (CS):           0.007988\n",
       "Covariance Type:                    nonrobust                                         \n",
       "====================================================================================================\n",
       "                                       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------\n",
       "section_114_flag_at_end_of_study    -1.7511      0.066    -26.463      0.000      -1.881      -1.621\n",
       "charity_has_land                    -0.6143      0.138     -4.438      0.000      -0.886      -0.343\n",
       "====================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_covariates = [\n",
    "    'section_114_flag_at_end_of_study',\n",
    "    'charity_has_land',\n",
    "]\n",
    "\n",
    "# --- 7. Prepare X (features) and y (target) ---\n",
    "X = final_df_for_logistic[model_covariates]\n",
    "y = final_df_for_logistic['is_removed_by_end_of_study']\n",
    "\n",
    "# Drop rows with NaNs in X or y\n",
    "initial_rows = len(X)\n",
    "data_for_model = pd.concat([X, y], axis=1).dropna()\n",
    "X_clean = data_for_model[model_covariates]\n",
    "y_clean = data_for_model['is_removed_by_end_of_study']\n",
    "print(f\"Dropped {initial_rows - len(X_clean)} rows due to NaNs in model features or target.\")\n",
    "\n",
    "# Add a constant (intercept) to the features for statsmodels\n",
    "X_clean = sm.add_constant(X_clean, prepend=False) # prepend=False puts it at the end\n",
    "\n",
    "print(f\"\\n--- Data for Logistic Regression (First 5 rows) ---\")\n",
    "print(pd.concat([X_clean, y_clean], axis=1).head())\n",
    "print(f\"Number of observations for Logistic Regression: {len(X_clean)}\")\n",
    "print(f\"Number of 'removed' outcomes: {y_clean.sum()}\")\n",
    "print(f\"Number of 'not removed' outcomes: {(y_clean == 0).sum()}\")\n",
    "\n",
    "# Convert 'True'/'False' strings or bools to 1/0\n",
    "X_clean['charity_has_land'] = X_clean['charity_has_land'].astype(bool).astype(int)\n",
    "\n",
    "print(X_clean.dtypes)\n",
    "print(y_clean.dtype)\n",
    "\n",
    "# --- 8. Fit the Logistic Regression Model ---\n",
    "# Use GLM with Binomial family for Logistic Regression\n",
    "model = GLM(y_clean, X_clean, family=families.Binomial())\n",
    "logistic_results = model.fit()\n",
    "\n",
    "print(\"\\n--- Logistic Regression Model Summary ---\")\n",
    "logistic_results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d3e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067477cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "birmingham_disposal.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdd344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from statsmodels.genmod import families\n",
    "import matplotlib.pyplot as plt # For potential plotting\n",
    "import warnings\n",
    "\n",
    "# --- 4. Prepare static covariates ---\n",
    "# Log transform income, handle potential 0s or NaNs gracefully\n",
    "df_bham_charities['charity_income_log'] = np.log(df_bham_charities['latest_income'].replace(0, 1) + 1) # Add 1 to avoid log(0) and handle 0 incomes\n",
    "#df_bham_charities['charity_has_land'] = df_bham_charities['charity_has_land'].astype(int) # Ensure integer type\n",
    "\n",
    "# Convert charity_type to dummy variables\n",
    "# statsmodels formula interface handles this automatically, but explicitly is good for control\n",
    "df_bham_charities = pd.get_dummies(df_bham_charities, columns=['charity_type'], prefix='charity_type', drop_first=True) # drop_first avoids multicollinearity\n",
    "\n",
    "# Note: Activity flags are already binary (0/1) from your data.\n",
    "\n",
    "# --- 5. Prepare snapshot time-varying covariates ---\n",
    "\n",
    "# Section 114 flag at end_of_study_date\n",
    "df_bham_charities['section_114_flag_at_end_of_study'] = int(s114_date <= end_of_study_date)\n",
    "\n",
    "# Cumulative disposals up to end_of_study_date\n",
    "birmingham_disposal['DISPLDATE'] = pd.to_datetime(birmingham_disposal['DISPLDATE'])\n",
    "\n",
    "# Filter disposals up to the end_of_study_date\n",
    "relevant_disposals_at_end = birmingham_disposal[\n",
    "    (birmingham_disposal['DISPLDATE'] <= end_of_study_date)\n",
    "].copy()\n",
    "\n",
    "# Calculate cumulative sums for these disposals\n",
    "# Add a small value to DEEDCONS before log transform to handle potential 0s if they existed\n",
    "relevant_disposals_at_end['DEEDCONS_log'] = np.log(relevant_disposals_at_end['DEEDCONS'] + 0.01)\n",
    "\n",
    "# Sum up the cumulative values at the end of the study. Since this is for a static model,\n",
    "# each charity gets the same total sum of disposals *up to that point in time*.\n",
    "# If you wanted to relate disposals to specific charities (e.g., if you had property links),\n",
    "# this would be more complex. For now, it's a general \"state of disposals in Birmingham.\"\n",
    "cumulative_bham_disposals_log_value_at_end = relevant_disposals_at_end['DEEDCONS_log'].sum()\n",
    "cumulative_disposal_type_community_centre_at_end = (relevant_disposals_at_end['TYPEPROP'] == 'Community Centre').sum()\n",
    "cumulative_disposal_type_leisure_facility_at_end = (relevant_disposals_at_end['TYPEPROP'] == 'Leisure Facility').sum()\n",
    "cumulative_disposal_type_office_at_end = (relevant_disposals_at_end['TYPEPROP'] == 'Office').sum()\n",
    "cumulative_disposal_type_land_at_end = (relevant_disposals_at_end['TYPEPROP'] == 'Land').sum()\n",
    "cumulative_disposal_method_community_transfer_at_end = (relevant_disposals_at_end['TYPECONSD'] == 'Community Transfer').sum()\n",
    "\n",
    "\n",
    "# Assign these as new columns to your main charities DataFrame\n",
    "df_bham_charities['cumulative_bham_disposals_log_value_at_end'] = cumulative_bham_disposals_log_value_at_end\n",
    "df_bham_charities['cumulative_disposal_type_community_centre_at_end'] = cumulative_disposal_type_community_centre_at_end\n",
    "df_bham_charities['cumulative_disposal_type_leisure_facility_at_end'] = cumulative_disposal_type_leisure_facility_at_end\n",
    "df_bham_charities['cumulative_disposal_type_office_at_end'] = cumulative_disposal_type_office_at_end\n",
    "df_bham_charities['cumulative_disposal_type_land_at_end'] = cumulative_disposal_type_land_at_end\n",
    "df_bham_charities['cumulative_disposal_method_community_transfer_at_end'] = cumulative_disposal_method_community_transfer_at_end\n",
    "\n",
    "\n",
    "# --- 6. Select final features for the model ---\n",
    "# Drop columns that were problematic (zero variance) or are no longer needed\n",
    "final_df_for_logistic = df_bham_charities.copy()\n",
    "\n",
    "# List of all covariates we want to include\n",
    "# Note: Removed the activity flags that were all 0s in your DUMMY data.\n",
    "# If your real data for these has variance, you can add them back.\n",
    "# 'Grants_Related', 'Housing_Infrastructure', 'Human_Rights_Advocacy',\n",
    "# 'Other', 'Other_Miscellaneous', 'Overseas_Famine_Relief', 'Religion_Faith'\n",
    "# 'Children_Youth' also had low variance in dummy data, but check your real data for it.\n",
    "# For now, let's stick to the common ones:\n",
    "model_covariates = [\n",
    "    'section_114_flag_at_end_of_study',\n",
    "    'cumulative_bham_disposals_log_value_at_end',\n",
    "    'cumulative_disposal_type_community_centre_at_end',\n",
    "    'cumulative_disposal_type_leisure_facility_at_end',\n",
    "    'cumulative_disposal_type_office_at_end',\n",
    "    'cumulative_disposal_type_land_at_end',\n",
    "    'cumulative_disposal_method_community_transfer_at_end',\n",
    "    'charity_income_log',\n",
    "    'charity_has_land',\n",
    "    # Add other activity flags if they have sufficient variance in your real data\n",
    "]\n",
    "\n",
    "# Check for existence of columns and drop if they don't exist (e.g., if get_dummies didn't create them)\n",
    "# This loop also removes the problematic zero-variance ones *if* they still exist\n",
    "# and are problematic in the real data.\n",
    "actual_model_covariates = []\n",
    "for col in model_covariates:\n",
    "    if col in final_df_for_logistic.columns:\n",
    "        # Check variance before adding to final list\n",
    "        if final_df_for_logistic[col].nunique() > 1: # Must have more than one unique value to vary\n",
    "            actual_model_covariates.append(col)\n",
    "        else:\n",
    "            print(f\"Skipping '{col}' due to zero or very low variance.\")\n",
    "    else:\n",
    "        print(f\"Warning: '{col}' not found in DataFrame. Check column creation or spelling.\")\n",
    "\n",
    "# --- 7. Prepare X (features) and y (target) ---\n",
    "X = final_df_for_logistic[actual_model_covariates]\n",
    "y = final_df_for_logistic['is_removed_by_end_of_study']\n",
    "\n",
    "# Drop rows with NaNs in X or y\n",
    "initial_rows = len(X)\n",
    "data_for_model = pd.concat([X, y], axis=1).dropna()\n",
    "X_clean = data_for_model[actual_model_covariates]\n",
    "y_clean = data_for_model['is_removed_by_end_of_study']\n",
    "print(f\"Dropped {initial_rows - len(X_clean)} rows due to NaNs in model features or target.\")\n",
    "\n",
    "# Add a constant (intercept) to the features for statsmodels\n",
    "X_clean = sm.add_constant(X_clean, prepend=False) # prepend=False puts it at the end\n",
    "\n",
    "print(f\"\\n--- Data for Logistic Regression (First 5 rows) ---\")\n",
    "print(pd.concat([X_clean, y_clean], axis=1).head())\n",
    "print(f\"Number of observations for Logistic Regression: {len(X_clean)}\")\n",
    "print(f\"Number of 'removed' outcomes: {y_clean.sum()}\")\n",
    "print(f\"Number of 'not removed' outcomes: {(y_clean == 0).sum()}\")\n",
    "\n",
    "# --- 8. Fit the Logistic Regression Model ---\n",
    "try:\n",
    "    # Use GLM with Binomial family for Logistic Regression\n",
    "    model = GLM(y_clean, X_clean, family=families.Binomial())\n",
    "    logistic_results = model.fit()\n",
    "\n",
    "    print(\"\\n--- Logistic Regression Model Summary ---\")\n",
    "    print(logistic_results.summary())\n",
    "\n",
    "    # --- Interpretation Tips ---\n",
    "    print(\"\\n--- Interpretation Notes ---\")\n",
    "    print(\"Coefficients (coef) are in log-odds. To get Odds Ratios (OR), exponentiate them (e.g., np.exp(coef)).\")\n",
    "    print(\"OR > 1 means an increase in the covariate is associated with higher odds of removal.\")\n",
    "    print(\"OR < 1 means an increase in the covariate is associated with lower odds of removal.\")\n",
    "    print(\"P>|z| (p-value) indicates statistical significance. Typically, < 0.05 is considered significant.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- Error during Model Fitting ---\")\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"This could still be due to perfect collinearity or extremely sparse data in a specific categorical variable.\")\n",
    "    print(\"Review the 'Skipping ... due to zero or very low variance' messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec97cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['date_of_registration'] = pd.to_datetime(dataset['date_of_registration'])\n",
    "dataset['date_of_removal'] = pd.to_datetime(dataset['date_of_removal'])\n",
    "dataset['registration_month'] = dataset['date_of_registration'].dt.to_period('M')\n",
    "dataset['removal_month'] = dataset['date_of_removal'].dt.to_period('M')\n",
    "\n",
    "removed_by_month_la = (\n",
    "    dataset\n",
    "    .groupby(['local_authority', 'removal_month'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .sort_index(axis=1)\n",
    ")\n",
    "panel_df = removed_by_month_la.reset_index().melt(\n",
    "    id_vars='local_authority',\n",
    "    var_name='month',\n",
    "    value_name='removed'\n",
    ")\n",
    "\n",
    "# Convert 'month' back to Period type for date calculations\n",
    "panel_df['month'] = panel_df['month'].astype(str).apply(pd.Period)\n",
    "\n",
    "# Add treatment info\n",
    "treatment_map = dataset.groupby('local_authority')[['treatment', 'treatment_date']].first()\n",
    "panel_df = panel_df.merge(treatment_map, on='local_authority', how='left')\n",
    "\n",
    "# Compute 'post' indicator at monthly granularity\n",
    "panel_df['treatment_month'] = panel_df['treatment_date'].dt.to_period('M')\n",
    "panel_df['post'] = (panel_df['month'] >= panel_df['treatment_month']).astype(int)\n",
    "panel_df.loc[panel_df['treatment'].isna(), 'post'] = 0  # untreated always 0\n",
    "\n",
    "# Convert month Period to string for fixed effects\n",
    "panel_df['month_str'] = panel_df['month'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1951ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build formula with month fixed effects\n",
    "formula = (\n",
    "    'removed ~ post + treatment + treatment:post + '\n",
    "    'C(local_authority) ' \n",
    ")\n",
    "\n",
    "model = smf.ols(formula, data=panel_df).fit()\n",
    "\n",
    "# Extract the coefficient table as a DataFrame\n",
    "summary_df = model.summary2().tables[1]\n",
    "\n",
    "# Filter for only the terms of interest\n",
    "terms_to_show = ['post', 'treatment', 'treatment:post']\n",
    "filtered_summary = summary_df.loc[summary_df.index.isin(terms_to_show)]\n",
    "\n",
    "print(filtered_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69da5c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Sort by local authority and month\n",
    "panel_df = panel_df.sort_values(['local_authority', 'month'])\n",
    "\n",
    "# First differences\n",
    "panel_df['D_removed'] = panel_df.groupby('local_authority')['removed'].diff()\n",
    "\n",
    "# Treatment is constant per group; post varies over time, so we create an interaction\n",
    "# This is the DiD estimator in differenced form\n",
    "interaction = panel_df['post'] * panel_df['treatment']\n",
    "panel_df['D_interaction'] = interaction.groupby(panel_df['local_authority']).diff()\n",
    "\n",
    "# Drop rows with missing differences\n",
    "panel_df_fd = panel_df.dropna(subset=['D_removed', 'D_interaction'])\n",
    "\n",
    "# Run first-difference regression\n",
    "fd_model = smf.ols('D_removed ~ D_interaction', data=panel_df_fd).fit()\n",
    "\n",
    "# Print the result\n",
    "print(fd_model.summary2().tables[1].loc[['D_interaction']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aed43c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by local authority and month\n",
    "panel_df = panel_df.sort_values(['local_authority', 'month'])\n",
    "\n",
    "# Avoid log(0) by replacing 0 with a small constant (or drop)\n",
    "panel_df['removed_adj'] = panel_df['removed'].replace(0, np.nan)\n",
    "\n",
    "# Log transform the outcome\n",
    "panel_df['log_removed'] = np.log(panel_df['removed_adj'])\n",
    "\n",
    "# First difference of log outcome (i.e., log-change â‰ˆ % change)\n",
    "panel_df['D_log_removed'] = panel_df.groupby('local_authority')['log_removed'].diff()\n",
    "\n",
    "# Interaction term: treatment x post, then differenced\n",
    "interaction = panel_df['post'] * panel_df['treatment']\n",
    "panel_df['D_interaction'] = interaction.groupby(panel_df['local_authority']).diff()\n",
    "\n",
    "# Drop missing values\n",
    "panel_df_fd = panel_df.dropna(subset=['D_log_removed', 'D_interaction'])\n",
    "\n",
    "# Fit model\n",
    "fd_model = smf.ols('D_log_removed ~ D_interaction', data=panel_df_fd).fit()\n",
    "\n",
    "# Print result\n",
    "print(fd_model.summary2().tables[1].loc[['D_interaction']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc88c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['date_of_registration'] = pd.to_datetime(dataset['date_of_registration'])\n",
    "dataset['date_of_removal'] = pd.to_datetime(dataset['date_of_removal'])\n",
    "dataset['registration_month'] = dataset['date_of_registration'].dt.to_period('M')\n",
    "dataset['removal_month'] = dataset['date_of_removal'].dt.to_period('M')\n",
    "\n",
    "# Define full monthly range (from min registration to max removal month)\n",
    "min_month = dataset['registration_month'].min()\n",
    "max_month = dataset['removal_month'].max()\n",
    "all_months = pd.period_range(min_month, max_month, freq='M')\n",
    "\n",
    "# Compute number of active charities at the end of each month by local authority\n",
    "active_by_month_la = {}\n",
    "\n",
    "for month in all_months:\n",
    "    # Active means registered before or on month AND not removed before or on month\n",
    "    active_mask = (\n",
    "        (dataset['registration_month'] <= month) &\n",
    "        (\n",
    "            dataset['removal_month'].isna() |\n",
    "            (dataset['removal_month'] > month)\n",
    "        )\n",
    "    )\n",
    "    active_count = (\n",
    "        dataset[active_mask]\n",
    "        .groupby('local_authority')\n",
    "        .size()\n",
    "        .rename(str(month))\n",
    "    )\n",
    "    active_by_month_la[month] = active_count\n",
    "\n",
    "active_by_month_la_df = pd.DataFrame(active_by_month_la).fillna(0).astype(int)\n",
    "\n",
    "# Count removals by month and local authority\n",
    "removed_by_month_la = (\n",
    "    dataset\n",
    "    .groupby(['local_authority', 'removal_month'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .sort_index(axis=1)\n",
    ")\n",
    "\n",
    "# Align indices of active and removal data\n",
    "active_by_month_la_df = active_by_month_la_df.reindex(removed_by_month_la.index)\n",
    "\n",
    "# Calculate removal rate per month\n",
    "removal_rate_by_month_la = removed_by_month_la / active_by_month_la_df\n",
    "\n",
    "# Convert wide to long format for panel data\n",
    "panel_df = removal_rate_by_month_la.reset_index().melt(\n",
    "    id_vars='local_authority',\n",
    "    var_name='month',\n",
    "    value_name='removed_rate'\n",
    ")\n",
    "\n",
    "# Convert 'month' back to Period type for date calculations\n",
    "panel_df['month'] = panel_df['month'].astype(str).apply(pd.Period)\n",
    "\n",
    "# Add treatment info\n",
    "treatment_map = dataset.groupby('local_authority')[['treatment', 'treatment_date']].first()\n",
    "panel_df = panel_df.merge(treatment_map, on='local_authority', how='left')\n",
    "\n",
    "# Compute 'post' indicator at monthly granularity\n",
    "panel_df['treatment_month'] = panel_df['treatment_date'].dt.to_period('M')\n",
    "panel_df['post'] = (panel_df['month'] >= panel_df['treatment_month']).astype(int)\n",
    "panel_df.loc[panel_df['treatment'].isna(), 'post'] = 0  # untreated always 0\n",
    "\n",
    "# Convert month Period to string for fixed effects\n",
    "panel_df['month_str'] = panel_df['month'].astype(str)\n",
    "\n",
    "# Build formula with month fixed effects\n",
    "formula = (\n",
    "    'removed_rate ~ post + treatment + treatment:post + '\n",
    "    'C(local_authority) ' \n",
    ")\n",
    "\n",
    "# Fit model with clustered standard errors by local authority\n",
    "model = smf.ols(formula, data=panel_df).fit()\n",
    "\n",
    "summary_df = model.summary()\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2532b9",
   "metadata": {},
   "source": [
    "YEARLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5093641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure date columns are datetime\n",
    "dataset['date_of_registration'] = pd.to_datetime(dataset['date_of_registration'])\n",
    "dataset['date_of_removal'] = pd.to_datetime(dataset['date_of_removal'])\n",
    "\n",
    "# Define FY end date mapping\n",
    "# Get financial years from 2010 to 2025\n",
    "fy_end_dates = (\n",
    "    dataset['removal_fy']\n",
    "    .dropna()\n",
    "    .astype(int)\n",
    ")\n",
    "fy_end_dates = sorted(fy_end_dates[(fy_end_dates >= 2014) & (fy_end_dates <= 2025)].unique())\n",
    "\n",
    "fy_end_date_map = {fy: pd.Timestamp(f\"{int(fy)+1}-03-31\") for fy in fy_end_dates}\n",
    "\n",
    "# Compute number of active charities at end of each FY by council\n",
    "active_by_fy_la = {}\n",
    "\n",
    "for fy, end_date in fy_end_date_map.items():\n",
    "    previous_end_date = end_date - pd.DateOffset(years=1)\n",
    "    active_mask = (\n",
    "        (dataset['date_of_registration'] <= previous_end_date) &\n",
    "        (\n",
    "            dataset['date_of_removal'].isna() |\n",
    "            (dataset['date_of_removal'] > previous_end_date)\n",
    "        )\n",
    "    )\n",
    "    active_count = (\n",
    "        dataset[active_mask]\n",
    "        .groupby('local_authority')\n",
    "        .size()\n",
    "        .rename(fy)\n",
    "    )\n",
    "    active_by_fy_la[fy] = active_count\n",
    "\n",
    "active_by_fy_la_df = pd.DataFrame(active_by_fy_la).fillna(0).astype(int)\n",
    "\n",
    "# Your original: removals by FY and local authority\n",
    "removed_by_fy_la = (\n",
    "    dataset\n",
    "    .groupby(['local_authority', 'removal_fy'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .sort_index(axis=1)\n",
    ")\n",
    "\n",
    "# Align indices\n",
    "active_by_fy_la_df = active_by_fy_la_df.reindex(removed_by_fy_la.index)\n",
    "\n",
    "# Calculate removal rate per FY\n",
    "removal_rate_by_fy_la = removed_by_fy_la / active_by_fy_la_df\n",
    "removal_rate_by_fy_la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a720b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Wide to long\n",
    "panel_df = removal_rate_by_fy_la.reset_index().melt(\n",
    "    id_vars='local_authority',\n",
    "    var_name='financial_year',\n",
    "    value_name='removed_rate'\n",
    ")\n",
    "\n",
    "# Step 2: Add treatment info\n",
    "# Map treatment and treatment_date from the original dataset\n",
    "treatment_map = dataset.groupby('local_authority')[['treatment', 'treatment_date']].first()\n",
    "panel_df = panel_df.merge(treatment_map, on='local_authority', how='left')\n",
    "\n",
    "# Step 3: Compute 'post' indicator\n",
    "panel_df['financial_year'] = panel_df['financial_year'].astype(int)\n",
    "panel_df['treatment_year'] = panel_df['treatment_date'].dt.year\n",
    "panel_df['post'] = (panel_df['financial_year'] >= panel_df['treatment_year']).astype(int)\n",
    "panel_df.loc[panel_df['treatment'].isna(), 'post'] = 0  # ensure untreated are always 0\n",
    "\n",
    "classification_vars = [\n",
    "    'Children_Youth', 'Economic_Social_Development', 'Education_Research',\n",
    "    'Environment_Animals', 'Grants_Related', 'Health_Disability',\n",
    "    'Housing_Infrastructure', 'Human_Rights_Advocacy',\n",
    "    'Other_Miscellaneous', 'Overseas_Famine_Relief', 'Religion_Faith'\n",
    "]\n",
    "classification_map = dataset.groupby('local_authority')[classification_vars].max()\n",
    "panel_df = panel_df.merge(classification_map, on='local_authority', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e91e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52098ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_df.dropna(subset=['removed_rate'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc115fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = (\n",
    "    'removed_rate ~ post + treatment + treatment:post + '\n",
    "    'C(local_authority) + C(financial_year) + ' +\n",
    "    ' + '.join(classification_vars)\n",
    ")\n",
    "\n",
    "model = smf.ols(formula, data=panel_df).fit(\n",
    "    cov_type='cluster', cov_kwds={'groups': panel_df['local_authority']}\n",
    ")\n",
    "summary = model.summary()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bbd0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# --- Assumes dataset is already loaded and 'dataset' is your DataFrame ---\n",
    "\n",
    "# Ensure date columns are datetime\n",
    "dataset['date_of_registration'] = pd.to_datetime(dataset['date_of_registration'])\n",
    "dataset['date_of_removal'] = pd.to_datetime(dataset['date_of_removal'])\n",
    "\n",
    "# --- Step 1: Create Financial Year End Dates ---\n",
    "fy_end_dates = (\n",
    "    dataset['removal_fy'].dropna().astype(int)\n",
    ")\n",
    "fy_end_dates = sorted(fy_end_dates[(fy_end_dates >= 2014) & (fy_end_dates <= 2025)].unique())\n",
    "fy_end_date_map = {fy: pd.Timestamp(f\"{int(fy)+1}-03-31\") for fy in fy_end_dates}\n",
    "\n",
    "# --- Step 2: Calculate Active Charities at Each FY End ---\n",
    "active_by_fy_la = {}\n",
    "for fy, end_date in fy_end_date_map.items():\n",
    "    prev_end_date = end_date - pd.DateOffset(years=1)\n",
    "    active_mask = (\n",
    "        (dataset['date_of_registration'] <= prev_end_date) &\n",
    "        (dataset['date_of_removal'].isna() | (dataset['date_of_removal'] > prev_end_date))\n",
    "    )\n",
    "    active_count = dataset[active_mask].groupby('local_authority').size().rename(fy)\n",
    "    active_by_fy_la[fy] = active_count\n",
    "\n",
    "active_by_fy_la_df = pd.DataFrame(active_by_fy_la).fillna(0).astype(int)\n",
    "\n",
    "# --- Step 3: Calculate Removals by FY ---\n",
    "removed_by_fy_la = (\n",
    "    dataset\n",
    "    .groupby(['local_authority', 'removal_fy'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .sort_index(axis=1)\n",
    ")\n",
    "\n",
    "# Align indices\n",
    "active_by_fy_la_df = active_by_fy_la_df.reindex(removed_by_fy_la.index)\n",
    "removal_rate_by_fy_la = removed_by_fy_la / active_by_fy_la_df\n",
    "\n",
    "# --- Step 4: Convert to Long Format Panel ---\n",
    "panel_df = removal_rate_by_fy_la.reset_index().melt(\n",
    "    id_vars='local_authority',\n",
    "    var_name='financial_year',\n",
    "    value_name='removed_rate'\n",
    ")\n",
    "\n",
    "# --- Step 5: Merge Treatment Info ---\n",
    "treatment_map = dataset.groupby('local_authority')[['treatment', 'treatment_date']].first()\n",
    "panel_df = panel_df.merge(treatment_map, on='local_authority', how='left')\n",
    "\n",
    "panel_df['financial_year'] = panel_df['financial_year'].astype(int)\n",
    "panel_df['treatment_year'] = panel_df['treatment_date'].dt.year\n",
    "panel_df['event_time'] = panel_df['financial_year'] - panel_df['treatment_year']\n",
    "\n",
    "# Create event study dummies with safe names\n",
    "for i in range(-5, 6):\n",
    "    if i != -1:  # omit -1 as base\n",
    "        col_name = f\"event_m{abs(i)}\" if i < 0 else f\"event_{i}\"\n",
    "        panel_df[col_name] = (panel_df['event_time'] == i).astype(int)\n",
    "\n",
    "# --- Step 6: Add lag of outcome variable ---\n",
    "panel_df = panel_df.sort_values(['local_authority', 'financial_year'])\n",
    "panel_df['removed_rate_lag1'] = panel_df.groupby('local_authority')['removed_rate'].shift(1)\n",
    "\n",
    "# --- Step 7: Add classification controls ---\n",
    "classification_vars = [\n",
    "    'Children_Youth', 'Economic_Social_Development', 'Education_Research',\n",
    "    'Environment_Animals', 'Grants_Related', 'Health_Disability',\n",
    "    'Housing_Infrastructure', 'Human_Rights_Advocacy',\n",
    "    'Other_Miscellaneous', 'Overseas_Famine_Relief', 'Religion_Faith'\n",
    "]\n",
    "classification_map = dataset.groupby('local_authority')[classification_vars].max()\n",
    "panel_df = panel_df.merge(classification_map, on='local_authority', how='left')\n",
    "\n",
    "# --- Step 8: Pre-trend interaction (optional) ---\n",
    "panel_df['pretrend'] = panel_df['financial_year'] - panel_df['financial_year'].min()\n",
    "panel_df['pretrend_treat'] = panel_df['pretrend'] * panel_df['treatment']\n",
    "panel_df_model = panel_df.dropna(subset=['removed_rate', 'removed_rate_lag1'])\n",
    "\n",
    "# --- Step 9: Build formula ---\n",
    "event_terms = [f'event_m{abs(i)}' if i < 0 else f'event_{i}' for i in range(-5, 6) if i != -1]\n",
    "\n",
    "formula = (\n",
    "    'removed_rate ~ removed_rate_lag1 + pretrend_treat + ' +\n",
    "    ' + '.join(event_terms) +\n",
    "    ' + C(local_authority) + C(financial_year) + ' +\n",
    "    ' + '.join(classification_vars)\n",
    ")\n",
    "# --- Step 10: Fit OLS with clustered standard errors ---\n",
    "model = smf.ols(formula=formula, data=panel_df_model).fit(\n",
    "    cov_type='cluster', cov_kwds={'groups': panel_df_model['local_authority']}\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36da072",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ADF: remove Non-stationary series (local councils)\n",
    "'''\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "results = {}\n",
    "for la in panel_df['local_authority'].unique():\n",
    "    series = panel_df.loc[panel_df['local_authority'] == la, 'removed_rate'].dropna()\n",
    "    if len(series) > 10:  # require enough data points\n",
    "        result = adfuller(series)\n",
    "        results[la] = {'ADF Statistic': result[0], 'p-value': result[1]}\n",
    "        \n",
    "for la, res in results.items():\n",
    "    print(f\"Local Authority: {la}, ADF Stat: {res['ADF Statistic']:.4f}, p-value: {res['p-value']:.4f}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your dictionary is stored in variable `results`\n",
    "# Flatten the dictionary into a list of rows for DataFrame construction\n",
    "rows = []\n",
    "for la, stats in results.items():\n",
    "    rows.append({\n",
    "        'local_authority': la,\n",
    "        'adf_stat': stats['ADF Statistic'],\n",
    "        'p_value': stats['p-value']\n",
    "    })\n",
    "\n",
    "df_adf = pd.DataFrame(rows)\n",
    "\n",
    "# Define thresholds for problematic series\n",
    "threshold_adf_stat = 1e10\n",
    "\n",
    "problematic = df_adf[\n",
    "    (df_adf['p_value'] == 0) |\n",
    "    (df_adf['p_value'] == 1) |\n",
    "    (df_adf['adf_stat'].abs() > threshold_adf_stat)\n",
    "]\n",
    "\n",
    "print(\"Problematic Local Authorities:\")\n",
    "print(problematic['local_authority'].tolist())\n",
    "\n",
    "# Optional: Filter them out from your panel_df if you want clean data\n",
    "clean_panel_df = panel_df[~panel_df['local_authority'].isin(problematic['local_authority'])]\n",
    "\n",
    "print(f\"Clean panel data now has {clean_panel_df['local_authority'].nunique()} local authorities.\")\n",
    "\n",
    "# Keep only stationary series (p-value < 0.05)\n",
    "stationary = df_adf[df_adf['p_value'] < 0.05]\n",
    "\n",
    "# Filter panel data accordingly\n",
    "stationary_panel_df = panel_df[panel_df['local_authority'].isin(stationary['local_authority'])]\n",
    "\n",
    "print(f\"Panel data now contains {stationary_panel_df['local_authority'].nunique()} stationary local authorities.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e04826",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = (\n",
    "    'removed_rate ~ removed_rate_lag1 + pretrend_treat + ' +\n",
    "    ' + '.join(event_terms) +\n",
    "    ' + C(local_authority) + C(financial_year) + ' +\n",
    "    ' + '.join(classification_vars)\n",
    ")\n",
    "# --- Step 10: Fit OLS with clustered standard errors ---\n",
    "model = smf.ols(formula=formula, data=panel_df_model).fit(\n",
    "    cov_type='cluster', cov_kwds={'groups': panel_df_model['local_authority']}\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b0d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter event leads (negative events)\n",
    "pre_treatment_events = [col for col in panel_df.columns if col.startswith('event_m')]\n",
    "\n",
    "# Extract their coefficients and p-values from your model\n",
    "coef_table = model.summary2().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714839ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Check Parallel Trends (Pre-treatment event coefficients)\n",
    "Treatment Effect Timing (Post-treatment event coefficients)\n",
    "'''\n",
    "\n",
    "# Filter event leads (negative events)\n",
    "pre_treatment_events = [col for col in panel_df.columns if col.startswith('event_m')]\n",
    "\n",
    "# Extract their coefficients and p-values from your model\n",
    "coef_table = model.summary2().tables[1]\n",
    "\n",
    "for ev in pre_treatment_events:\n",
    "    coef = coef_table.loc[ev, 'Coef.']\n",
    "    pval = coef_table.loc[ev, 'P>|z|']\n",
    "    print(f\"{ev}: Coef = {coef:.4f}, p-value = {pval:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e73729",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Coefficients on Classification Groups\n",
    "'''\n",
    "classification_vars = [\n",
    "    'Children_Youth', 'Economic_Social_Development', 'Education_Research',\n",
    "    'Environment_Animals', 'Grants_Related', 'Health_Disability',\n",
    "    'Housing_Infrastructure', 'Human_Rights_Advocacy',\n",
    "    'Other_Miscellaneous', 'Overseas_Famine_Relief', 'Religion_Faith'\n",
    "]\n",
    "\n",
    "for var in classification_vars:\n",
    "    coef = coef_table.loc[var, 'Coef.']\n",
    "    pval = coef_table.loc[var, 'P>|z|']\n",
    "    print(f\"{var}: Coef = {coef:.4f}, p-value = {pval:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d55c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Residuals vs fitted values plot\n",
    "fitted_vals = model.fittedvalues\n",
    "residuals = model.resid\n",
    "\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "dw_stat = durbin_watson(residuals)\n",
    "print(f\"Durbin-Watson statistic: {dw_stat:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96177a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract event coefficients and std errors from your model summary\n",
    "coef_table = model.summary2().tables[1]\n",
    "\n",
    "# Select event terms (both leads and lags)\n",
    "event_terms = [col for col in coef_table.index if col.startswith('event')]\n",
    "\n",
    "# Extract coefficients and standard errors\n",
    "coefs = coef_table.loc[event_terms, 'Coef.']\n",
    "std_errs = coef_table.loc[event_terms, 'Std.Err.']\n",
    "\n",
    "# Calculate 95% confidence intervals\n",
    "ci_upper = coefs + 1.96 * std_errs\n",
    "ci_lower = coefs - 1.96 * std_errs\n",
    "\n",
    "# Convert event names to numeric x-axis values\n",
    "def event_to_num(ev):\n",
    "    if ev.startswith('event_m'):\n",
    "        return -int(ev.split('_m')[1])\n",
    "    else:\n",
    "        return int(ev.split('_')[1])\n",
    "\n",
    "x_vals = [event_to_num(ev) for ev in event_terms]\n",
    "\n",
    "# Sort by x-axis for plotting\n",
    "sorted_indices = sorted(range(len(x_vals)), key=lambda i: x_vals[i])\n",
    "x_sorted = [x_vals[i] for i in sorted_indices]\n",
    "coefs_sorted = [coefs[i] for i in sorted_indices]\n",
    "ci_upper_sorted = [ci_upper[i] for i in sorted_indices]\n",
    "ci_lower_sorted = [ci_lower[i] for i in sorted_indices]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x_sorted, coefs_sorted, marker='o', label='Coefficient')\n",
    "plt.fill_between(x_sorted, ci_lower_sorted, ci_upper_sorted, color='blue', alpha=0.2, label='95% CI')\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.axvline(0, color='grey', linestyle='--')\n",
    "plt.xlabel('Event Time (Years relative to treatment)')\n",
    "plt.ylabel('Coefficient')\n",
    "plt.title('Event Study: Lead and Lag Coefficients')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3a34e",
   "metadata": {},
   "source": [
    "DRAFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate total 2024 housing stock per council (Local authority only)\n",
    "stock_2024 = (\n",
    "    housing_stock_df\n",
    "    .groupby(['Authority_code', 'Authority_name'], as_index=False)['LA_number']\n",
    "    .sum()\n",
    "    .rename(columns={'LA_number': 'Stock_2024'})\n",
    ")\n",
    "\n",
    "# Step 2: Prepare Net Supply Data\n",
    "net = housing_net_supply_df.copy()\n",
    "net = net[net['Year'].astype(str).str.match(r'^\\d{4}/\\d{2}$')]  # Keep only year-formatted rows\n",
    "net['Year'] = net['Year'].str[:4].astype(int)  # Convert '2015/16' â†’ 2015\n",
    "\n",
    "# Pivot to Local Authority x Year format\n",
    "net_pivot = net.pivot(index='LA_code', columns='Year', values='Net_additions').fillna(0)\n",
    "net_pivot = net_pivot[sorted(net_pivot.columns, reverse=True)]  # descending order\n",
    "\n",
    "# Step 3: Merge with 2024 stock\n",
    "stock = stock_2024.set_index('Authority_code')\n",
    "net_pivot = net_pivot.reindex(stock.index)  # align index\n",
    "\n",
    "# Step 4: Calculate total stock backwards from 2024\n",
    "stock_by_year = pd.DataFrame(index=net_pivot.index)\n",
    "stock_by_year[2024] = stock['Stock_2024']\n",
    "\n",
    "for year in sorted(net_pivot.columns, reverse=True):\n",
    "    if year < 2024:\n",
    "        stock_by_year[year] = stock_by_year[year + 1] - net_pivot[year]\n",
    "\n",
    "# Step 5: Merge back Authority name and reshape\n",
    "stock_by_year = stock_by_year.merge(stock[['Authority_name']], left_index=True, right_index=True)\n",
    "stock_by_year_reset = stock_by_year.reset_index().melt(id_vars=['Authority_code', 'Authority_name'], \n",
    "                                                       var_name='Year', value_name='Housing stock')\n",
    "\n",
    "# Final pivot\n",
    "stock_by_year_pivot = stock_by_year_reset.pivot(index=['Authority_code', 'Authority_name'], \n",
    "                                                 columns='Year', values='Housing stock').sort_index(axis=1)\n",
    "\n",
    "# Reset index, rename, and set it again\n",
    "stock_by_year_pivot = stock_by_year_pivot.reset_index().rename(columns={'Authority_code': 'ONS code'})\n",
    "stock_by_year_pivot = stock_by_year_pivot.set_index(['ONS code', 'Authority_name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820a5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter: Remove NaN and keep only rows where 'Year' matches 'YYYY/YY'\n",
    "ladataset_filtered = ladataset[\n",
    "    ladataset['Year'].notna() & ladataset['Year'].str.match(r'^\\d{4}/\\d{2}$')\n",
    "]\n",
    "ladataset_filtered['Year'] = ladataset_filtered['Year'].str[:4].astype(int)\n",
    "\n",
    "# Pivot\n",
    "ladataset_pivot = ladataset_filtered.pivot_table(\n",
    "    index=['ONS code', 'Local authority', 'Measure'],\n",
    "    columns='Year',\n",
    "    values='Â£ millions, cash terms',\n",
    "    aggfunc='sum'\n",
    ").reset_index()\n",
    "\n",
    "# Group by 'con_code' only\n",
    "population_summary = (\n",
    "    population\n",
    "    .groupby('con_code', as_index=False)\n",
    "    .agg({'con_name': 'first', 'con_number': 'sum'})\n",
    "    .rename(columns={'con_name': 'Local authority', 'con_number': 'Total Population'})\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(population_summary.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f4866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Core Spending Power (CSP)\n",
    "csp = ladataset_pivot.copy()\n",
    "\n",
    "# Keep only relevant years\n",
    "csp = csp.set_index(['ONS code', 'Local authority']).drop(columns='Measure')\n",
    "\n",
    "# Calculate % change from 2015\n",
    "csp['financial_distress'] = (csp[2024] - csp[2015]) / csp[2015]\n",
    "\n",
    "fd = csp[['financial_distress']].reset_index()\n",
    "\n",
    "fd.columns.name = None\n",
    "housing_stock_processed = housing_stock_df[housing_stock_df['Tenure'] == 'Local authority'][['Authority_code', 'LA_number']]\n",
    "fd_new = fd.merge(housing_stock_processed, left_on='ONS code', right_on='Authority_code', how='inner')\n",
    "fd_new = fd_new.drop(columns='Authority_code')\n",
    "fd_new = fd_new.rename(columns={'LA_number': 'LA Housing stock 2024'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102acaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Normalise both variables\n",
    "fd_new['financial_distress_norm'] = (\n",
    "    (fd_new['financial_distress'] - fd_new['financial_distress'].min()) /\n",
    "    (fd_new['financial_distress'].max() - fd_new['financial_distress'].min())\n",
    ")\n",
    "\n",
    "fd_new['LA Housing stock 2024_norm'] = (\n",
    "    (fd_new['LA Housing stock 2024'] - fd_new['LA Housing stock 2024'].min()) /\n",
    "    (fd_new['LA Housing stock 2024'].max() - fd_new['LA Housing stock 2024'].min())\n",
    ")\n",
    "\n",
    "# Step 2: Avoid division by zero and calculate treatment\n",
    "fd_new['treatment'] = np.where(\n",
    "    fd_new['LA Housing stock 2024_norm'] != 0,\n",
    "    fd_new['financial_distress_norm'] / fd_new['LA Housing stock 2024_norm'],\n",
    "    0\n",
    ")\n",
    "fd_new.rename(columns={'Local authority': 'local_authority'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Filter valid rows\n",
    "removed = dataset[dataset['removal_fy'].notnull() & dataset['local_authority'].notnull()].copy()\n",
    "removed['removal_fy'] = removed['removal_fy'].astype(int)\n",
    "\n",
    "# Step 3: Group by financial year and local authority\n",
    "removed_by_fy_la = (\n",
    "    removed\n",
    "    .groupby(['local_authority', 'removal_fy'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .sort_index(axis=1)\n",
    ")\n",
    "\n",
    "# Step 4: Keep only FY 2015â€“2024\n",
    "fy_years = list(range(2015, 2025))\n",
    "removed_by_fy_la = removed_by_fy_la[removed_by_fy_la.columns.intersection(fy_years)]\n",
    "\n",
    "# Step 5: Total and sort\n",
    "removed_by_fy_la['Total'] = removed_by_fy_la.sum(axis=1)\n",
    "removed_by_fy_la = removed_by_fy_la.sort_values(by='Total', ascending=False)\n",
    "\n",
    "# Step 6: Preview top 10\n",
    "print(\"Removed Charities per Financial Year per Local Authority (FY 2015â€“2024):\")\n",
    "print(removed_by_fy_la.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a02ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define year range\n",
    "years = list(range(2015, 2025))\n",
    "\n",
    "# Step 2: Calculate percentage change across years\n",
    "removed_pct_change = removed_by_fy_la[years].pct_change(axis=1) * 100\n",
    "\n",
    "# Step 3: Round for readability\n",
    "removed_pct_change = removed_pct_change.round(2)\n",
    "removed_pct_change"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
