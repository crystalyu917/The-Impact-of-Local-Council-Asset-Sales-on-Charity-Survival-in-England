{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f43fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import itertools\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from statsmodels.tsa.stattools import grangercausalitytests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e88ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPOSAL_FILEPATH = \"../../data/raw/council_disposal_receipts.xlsx\"\n",
    "\n",
    "YEAR_SKIP_MAPPING = {\n",
    "    \"2014\": 0,\n",
    "    \"2015\": 0,\n",
    "    \"2016\": 2,\n",
    "    \"2017\": 3,\n",
    "    \"2018\": 3,\n",
    "    \"2019\": 4,\n",
    "    \"2020\": 4,\n",
    "    \"2021\": 4,\n",
    "    \"2022\": 4,\n",
    "    \"2023\": 4,\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "for year, skip_row in YEAR_SKIP_MAPPING.items():\n",
    "    df = pd.read_excel(DISPOSAL_FILEPATH, skiprows=skip_row, sheet_name=year)\n",
    "    dfs.append(df)\n",
    "\n",
    "dataset = pd.read_csv(r'../../data/processed/charity_main_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf1927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sheet(df, rename_from_col=3, header_row=1):\n",
    "    \"\"\"\n",
    "    Cleans a DataFrame read from Excel:\n",
    "    - Replaces values in `header_row` with column names starting from `rename_from_col`\n",
    "    - Keeps all columns up to `rename_from_col`\n",
    "    - Keeps only renamed columns after `rename_from_col`\n",
    "    - Sets row `header_row` as the header and drops all rows above it\n",
    "    \"\"\"\n",
    "    # Step 1: Identify renamed columns\n",
    "    renamed_mask = [not str(col).startswith(\"Unnamed:\") for col in df.columns]\n",
    "\n",
    "    # Step 2: Determine which columns to keep\n",
    "    cols_to_keep = list(range(rename_from_col + 1))  # Always keep up to and including `rename_from_col`\n",
    "    for i in range(rename_from_col + 1, len(df.columns)):\n",
    "        if renamed_mask[i]:\n",
    "            cols_to_keep.append(i)\n",
    "\n",
    "    # Step 3: Overwrite header_row with column names for renamed columns\n",
    "    for i in range(rename_from_col, len(df.columns)):\n",
    "        if renamed_mask[i]:\n",
    "            df.iloc[header_row, i] = df.columns[i]\n",
    "\n",
    "    # Step 4: Set row `header_row` as header and drop rows above\n",
    "    df.columns = df.iloc[header_row]\n",
    "    df = df.drop(index=list(range(header_row + 1))).reset_index(drop=True)\n",
    "\n",
    "    # Step 5: Keep only selected columns\n",
    "    df = df.iloc[:, cols_to_keep]\n",
    "\n",
    "    return df\n",
    "\n",
    "def rename_and_filter_disposal(df, start_col=5, keyword=\": Disposal of tangible fixed assets\"):\n",
    "    \"\"\"\n",
    "    Keeps:\n",
    "    - All columns before `start_col`\n",
    "    - Columns from `start_col` onward that contain `keyword`\n",
    "    \n",
    "    Renames matching columns by removing the keyword from their name.\n",
    "    \"\"\"\n",
    "    cols_to_keep = list(range(start_col))  # Keep early columns as-is\n",
    "\n",
    "    new_columns = df.columns.tolist()  # Copy of column names\n",
    "\n",
    "    for i in range(start_col, len(df.columns)):\n",
    "        col = str(df.columns[i])\n",
    "        if keyword in col:\n",
    "            # Rename column by removing the keyword\n",
    "            new_columns[i] = col.replace(keyword, \"\")\n",
    "            cols_to_keep.append(i)\n",
    "\n",
    "    # Apply renaming\n",
    "    df.columns = new_columns\n",
    "\n",
    "    # Keep only selected columns\n",
    "    return df.iloc[:, cols_to_keep]\n",
    "\n",
    "dfs[0] = clean_sheet(dfs[0], rename_from_col=3, header_row=1)\n",
    "dfs[1] = clean_sheet(dfs[1], rename_from_col=4, header_row=1)\n",
    "dfs[2] = clean_sheet(dfs[2], rename_from_col=4, header_row=2)\n",
    "dfs[3] = clean_sheet(dfs[3], rename_from_col=5, header_row=0)\n",
    "dfs[4] = clean_sheet(dfs[4], rename_from_col=5, header_row=0)\n",
    "\n",
    "dfs[5] = rename_and_filter_disposal(dfs[5], start_col=5)\n",
    "dfs[6] = rename_and_filter_disposal(dfs[6], start_col=5)\n",
    "dfs[7] = rename_and_filter_disposal(dfs[7], start_col=5)\n",
    "dfs[8] = rename_and_filter_disposal(dfs[8], start_col=5)\n",
    "dfs[9] = rename_and_filter_disposal(dfs[9], start_col=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73226bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    df = dfs[i]\n",
    "\n",
    "    # Step 1: Standardise column names first\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    if 'la name' in df.columns:\n",
    "        df = df.rename(columns={'la name': 'local_authority'})\n",
    "    \n",
    "    if 'local_authority' in df.columns:\n",
    "        df['local_authority'] = df['local_authority'].str.replace(r'(?i)\\(ua\\)|\\bua\\b', '', regex=True).str.strip()\n",
    "\n",
    "    # Step 2: Drop bad values in first column\n",
    "    first_col = df.columns[0]\n",
    "    df = df[~df[first_col].isin([pd.NA, None, '[z]', 'la_lgf_code'])]\n",
    "    df = df.dropna(subset=[first_col])\n",
    "\n",
    "    # Step 3: Drop missing ONS codes\n",
    "    if i == 0 and 'ecode' in df.columns:\n",
    "        df = df.dropna(subset=['ecode'])\n",
    "    elif i != 0 and 'ons code' in df.columns:\n",
    "        df = df.dropna(subset=['ons code'])\n",
    "\n",
    "    # Step 4: Drop unwanted council classes\n",
    "    if 'class' in df.columns:\n",
    "        df = df[~df['class'].isin(['O', 'MC','SC'])]\n",
    "\n",
    "    # Save cleaned frame back\n",
    "    dfs[i] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7184a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_columns = {\n",
    "    \"community safety\": [\"community safety\", \"community safety (cctv)\"],\n",
    "    'agricultural & fisheries services':['agriculture & fisheries'],\n",
    "    'all services total':['total all services'],\n",
    "    'early years & primary schools':['pre-primary & primary education'],\n",
    "    'parking':['parking of vehicles'],\n",
    "    'ports & piers':['local authority ports & piers'],\n",
    "    'special schools & alternative provision':['special education'],\n",
    "    'secondary schools':['secondary education'],\n",
    "    'total industrial & commercial trading':['industrial & commercial trading'],\n",
    "    'total environmental & regulatory services':['total environmental services','regulatory services (environmental health)'],\n",
    "    'tolled roads, bridges, tunnels,ferries & public transport companies':['tolled road bridges, tunnels, ferries, public transport companies','tolled road bridges, tunnels, ferries & public transport companies'],\n",
    "    'public transport (bus)':['public passenger transport - bus'],\n",
    "    'public transport (rail & other)':['public passenger transport - rail & other'],\n",
    "    'total housing':['housing'],\n",
    "    'total police':['police'],\n",
    "    'total social care':['social services','social care'],\n",
    "    'total public health':['public health'],\n",
    "    'roads, street lighting & road safety':['roads, street lights & safety'],\n",
    "    'total fire & rescue services':['fire & rescue services'],\n",
    "    'total central services':['central services (including court services)'],\n",
    "    'street cleaning (not chargeable to highways)':['street cleaning not chargeable to highways'],\n",
    "    'total planning & development':['total planning & development services','planning & development services'],\n",
    "    'total trading services':['total trading','trading'],\n",
    "    'total education':['education'],\n",
    "    'total highways & transport':['total transport','highways & transport'],\n",
    "    'total culture & related services':['culture & heritage']\n",
    "\n",
    "    #'commercial housing',\n",
    "    #'energy generation & supply',\n",
    "    #'finance & insurance activity',\n",
    "    #'hospitality & catering',\n",
    "    #'lgf code',\n",
    "    #'ons code',\n",
    "    #'other commercial activity',\n",
    "    #'other real estate activities',\n",
    "    #'post-16 provision & other education',\n",
    "    #'subclass',\n",
    "    #'total digital infrastructure',\n",
    "    #'water supply, sewerage & remediation'\n",
    "}\n",
    "\n",
    "# Standardise and merge alias columns across all dfs\n",
    "for idx in range(len(dfs)):\n",
    "    df = dfs[idx].copy()\n",
    "    df.columns = [\n",
    "        str(col).strip().lower()\n",
    "        .replace(\" and \", \" & \")\n",
    "        for col in df.columns\n",
    "    ]\n",
    "\n",
    "    dfs[idx] = df  # Save the cleaned version\n",
    "\n",
    "    for standard_col, aliases in merge_columns.items():\n",
    "        # Find which alias columns are present\n",
    "        present_cols = [col for col in aliases if col in df.columns]\n",
    "        if not present_cols:\n",
    "            continue  # Nothing to merge for this category\n",
    "\n",
    "        # Sum across all present columns\n",
    "        df[standard_col] = df[present_cols].sum(axis=1, skipna=True)\n",
    "\n",
    "        # Drop duplicates, except the one weâ€™re keeping\n",
    "        cols_to_drop = [col for col in present_cols if col != standard_col]\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    dfs[idx] = df  # Save the cleaned DataFrame back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bd720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined mapping: new_name â†’ list of old names\n",
    "unified_map = {\n",
    "    'buckinghamshire': ['aylesbury vale', 'chiltern', 'south bucks', 'wycombe','south buckinghamshire'],\n",
    "    'dorset': ['weymouth and portland', 'west dorset', 'north dorset', 'east dorset', 'purbeck', 'christchurch'],\n",
    "    'somerset': ['taunton deane', 'west somerset', 'mendip', 'sedgemoor', 'south somerset', 'somerset council','somerset west and taunton'],\n",
    "    'cumberland': ['allerdale', 'carlisle', 'copeland', 'cumberland council'],\n",
    "    'westmorland and furness': ['barrow in furness', 'barrow-in-furness', 'eden', 'south lakeland'],\n",
    "    'north yorkshire': ['craven', 'hambleton', 'harrogate', 'richmondshire', 'ryedale', 'scarborough', 'selby', 'north yorkshire council'],\n",
    "    'bournemouth christchurch and poole': ['bournemouth', 'christchurch', 'poole'],\n",
    "    'west suffolk': ['forest heath', 'st edmundsbury'],\n",
    "    'east suffolk': ['suffolk coastal', 'waveney'],\n",
    "    'bath and north east somerset': ['bath and ne somerset'],\n",
    "    'southend-on-sea': ['southend on sea'],\n",
    "    'leicester': ['leicester city'],\n",
    "    'medway': ['medway towns'],\n",
    "    'derby': ['derby city'],\n",
    "    'folkestone and hythe': ['shepway'],\n",
    "    'county durham': ['durham'],\n",
    "    \"king's lynn and west norfolk\": ['kings lynn and west norfolk'],\n",
    "    'north northamptonshire': ['wellingborough', 'east northamptonshire', 'kettering', 'corby'],\n",
    "    'west northamptonshire': ['northampton', 'south northamptonshire', 'daventry'],\n",
    "}\n",
    "# old_name â†’ new_name (lowercase)\n",
    "flat_lookup = {\n",
    "    old.lower(): new.lower()\n",
    "    for new, olds in unified_map.items()\n",
    "    for old in olds\n",
    "}\n",
    "def clean_local_authority(name):\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    name = str(name).lower()\n",
    "    name = (\n",
    "        name.replace('&', 'and')\n",
    "        .replace('-', ' ')\n",
    "        .replace(',', '')\n",
    "        .replace('.', '')\n",
    "        .replace(' city of', '')\n",
    "        .replace(' county of', '')\n",
    "        .strip()\n",
    "    )\n",
    "    return flat_lookup.get(name, name)\n",
    "# Apply to disposal files\n",
    "for i in range(len(dfs)):\n",
    "    if 'local_authority' in dfs[i].columns:\n",
    "        dfs[i]['local_authority'] = dfs[i]['local_authority'].apply(clean_local_authority)\n",
    "\n",
    "# Apply to charity dataset\n",
    "if 'local_authority' in dataset.columns:\n",
    "    dataset['local_authority'] = dataset['local_authority'].apply(clean_local_authority)\n",
    "\n",
    "# List of known devolved nation council names to exclude\n",
    "non_england_keywords = [\n",
    "    'aberdeen', 'aberdeenshire', 'angus', 'antrim', 'ards', 'argyll', 'armagh', 'belfast', 'blaenau', 'bridgend',\n",
    "    'caerphilly', 'cardiff', 'carmarthenshire', 'causeway', 'ceredigion', 'conwy', 'denbighshire', 'derry',\n",
    "    'dumfries', 'dundee', 'east ayrshire', 'east dunbartonshire', 'east lothian', 'east renfrewshire', 'falkirk',\n",
    "    'fermanagh', 'fife', 'flintshire', 'glasgow', 'gwynedd', 'highland', 'inverclyde', 'isle of man',\n",
    "    'isle of anglesey', 'lisburn', 'merthyr', 'mid and east antrim', 'mid ulster', 'midlothian', 'monmouthshire',\n",
    "    'moray', 'na h eileanan siar', 'neath', 'newport', 'newry', 'north ayrshire', 'north lanarkshire', 'orkney',\n",
    "    'pembrokeshire', 'perth and kinross', 'powys', 'renfrewshire', 'rhondda', 'scottish borders', 'shetland',\n",
    "    'south ayrshire', 'south lanarkshire', 'stirling', 'swansea', 'torfaen', 'vale of glamorgan', 'west dunbartonshire',\n",
    "    'west lothian', 'wrexham','city of edinburgh','channel islands'\n",
    "]\n",
    "\n",
    "# Convert to lowercase and filter out rows containing these names\n",
    "dataset['local_authority_lower'] = dataset['local_authority'].str.lower()\n",
    "dataset = dataset[~dataset['local_authority_lower'].str.contains('|'.join(non_england_keywords), na=False)]\n",
    "\n",
    "# Drop the helper column\n",
    "dataset = dataset.drop(columns=['local_authority_lower'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb290438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get unique local_authority names from dfs[0â€“9]\n",
    "all_disposal_names = set()\n",
    "for i in range(10):\n",
    "    if 'local_authority' in dfs[i].columns:\n",
    "        all_disposal_names.update(dfs[i]['local_authority'].dropna().unique())\n",
    "\n",
    "# Step 2: Get unique local_authority names from dataset\n",
    "if 'local_authority' in dataset.columns:\n",
    "    charity_names = set(dataset['local_authority'].dropna().unique())\n",
    "else:\n",
    "    raise KeyError(\"'local_authority' column not found in dataset\")\n",
    "\n",
    "# Step 3: Find mismatches\n",
    "in_disposal_not_in_charity = all_disposal_names - charity_names\n",
    "in_charity_not_in_disposal = charity_names - all_disposal_names\n",
    "\n",
    "# Step 4: Print results\n",
    "print(\"âœ… In disposal (dfs[0â€“9]) but NOT in charity dataset:\")\n",
    "print(sorted(in_disposal_not_in_charity))\n",
    "\n",
    "print(\"\\nğŸ” In charity dataset but NOT in disposal (dfs[0â€“9]):\")\n",
    "print(sorted(in_charity_not_in_disposal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6191049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Assign financial year to each DataFrame in dfs\n",
    "years = list(range(2014, 2024))\n",
    "for i, year in enumerate(years):\n",
    "    if not dfs[i].empty:\n",
    "        dfs[i]['financial_year'] = year\n",
    "\n",
    "# Step 2: Melt each DataFrame into long format\n",
    "long_frames = []\n",
    "for i, df in enumerate(dfs):\n",
    "    if df.empty:\n",
    "        continue\n",
    "    long_df = df.melt(\n",
    "        id_vars=[\"local_authority\", \"financial_year\"],\n",
    "        value_vars=[\n",
    "            col for col in df.columns \n",
    "            if col not in [\"ecode\", 'lgf code', 'ons code', \"class\", \"subclass\", \n",
    "                           \"local_authority\", \"financial_year\", 'certification complete']\n",
    "        ],\n",
    "        var_name=\"category\",\n",
    "        value_name=\"value\"\n",
    "    )\n",
    "    long_frames.append(long_df)\n",
    "\n",
    "# Step 3: Combine all disposal long-format DataFrames\n",
    "disposal_long_df = pd.concat(long_frames, ignore_index=True)\n",
    "\n",
    "# Step 4: Count removals from charity dataset\n",
    "removals = (\n",
    "    dataset\n",
    "    .groupby(['local_authority', 'removal_fy', 'size_category'])\n",
    "    .size()\n",
    "    .reset_index(name='removals')\n",
    "    .rename(columns={'removal_fy': 'financial_year'})\n",
    ")\n",
    "\n",
    "# Step 5: Merge disposal data with charity removals\n",
    "panel = pd.merge(\n",
    "    disposal_long_df,\n",
    "    removals,\n",
    "    on=[\"financial_year\", \"local_authority\"],\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "# Step 6: Replace NaNs in 'removals' with 0\n",
    "panel['removals'] = panel['removals'].fillna(0).astype(int)\n",
    "panel = panel[(panel['financial_year'] >= 2015) & (panel['financial_year'] <= 2023)]\n",
    "panel['value'] = pd.to_numeric(panel['value'], errors='coerce') / 1000000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Separate disposal data (no size_category)\n",
    "disposal = panel[['local_authority', 'financial_year', 'category', 'value']].drop_duplicates()\n",
    "\n",
    "# Step 2: Separate charity data (includes size_category)\n",
    "charity = panel[['local_authority', 'financial_year', 'category', 'size_category', 'removals']]\n",
    "\n",
    "# Step 3: Generate full index for all combinations\n",
    "all_authorities = panel['local_authority'].unique()\n",
    "all_years = panel['financial_year'].unique()\n",
    "all_sizes = ['Small', 'Medium', 'Large']\n",
    "all_categories = panel['category'].unique()\n",
    "\n",
    "full_index = pd.DataFrame(\n",
    "    list(itertools.product(all_authorities, all_years, all_categories, all_sizes)),\n",
    "    columns=['local_authority', 'financial_year', 'category', 'size_category']\n",
    ")\n",
    "\n",
    "# Step 4: Merge disposal (no size_category, shared across sizes)\n",
    "merged = pd.merge(\n",
    "    full_index,\n",
    "    disposal,\n",
    "    on=['local_authority', 'financial_year', 'category'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Step 5: Merge removals\n",
    "merged = pd.merge(\n",
    "    merged,\n",
    "    charity,\n",
    "    on=['local_authority', 'financial_year', 'category', 'size_category'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Step 6: Fill missing values\n",
    "merged['value'] = pd.to_numeric(merged['value'], errors='coerce').fillna(0)\n",
    "merged['removals'] = merged['removals'].fillna(0).astype(int)\n",
    "\n",
    "# Resulting panel\n",
    "panel_complete = merged\n",
    "panel_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a6a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulting filtered panel\n",
    "filtered_panel = panel_complete[panel_complete['category']=='all services total']\n",
    "# Ensure proper sorting\n",
    "filtered_panel = filtered_panel.sort_values(by=['local_authority', 'size_category', 'financial_year'])\n",
    "\n",
    "# Create lagged value columns\n",
    "filtered_panel['value_lag1'] = (\n",
    "    filtered_panel\n",
    "    .groupby(['local_authority', 'size_category'])['value']\n",
    "    .shift(1)\n",
    ")\n",
    "\n",
    "filtered_panel['value_lag2'] = (\n",
    "    filtered_panel\n",
    "    .groupby(['local_authority', 'size_category'])['value']\n",
    "    .shift(2)\n",
    ")\n",
    "\n",
    "filtered_panel['value_lag3'] = (\n",
    "    filtered_panel\n",
    "    .groupby(['local_authority', 'size_category'])['value']\n",
    "    .shift(3)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede2792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF test on 'removals'\n",
    "result_removals = adfuller(filtered_panel['removals'].dropna())\n",
    "print(\"ADF Test for 'removals':\")\n",
    "print(f\"ADF Statistic: {result_removals[0]}\")\n",
    "print(f\"p-value: {result_removals[1]}\")\n",
    "print(f\"Critical Values: {result_removals[4]}\\n\")\n",
    "\n",
    "# ADF test on 'value'\n",
    "result_value = adfuller(filtered_panel['value'].dropna())\n",
    "print(\"ADF Test for 'value':\")\n",
    "print(f\"ADF Statistic: {result_value[0]}\")\n",
    "print(f\"p-value: {result_value[1]}\")\n",
    "print(f\"Critical Values: {result_value[4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4291d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Engle-Granger cointegration test\n",
    "coint_stat, p_value, crit_values = coint(filtered_panel['removals'], filtered_panel['value'])\n",
    "\n",
    "print(f\"Engle-Granger Cointegration Test\")\n",
    "print(f\"Test Statistic: {coint_stat}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "print(f\"Critical Values: {crit_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999e5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data as a two-column array: [removals, value]\n",
    "data = filtered_panel[['removals', 'value']].dropna()\n",
    "\n",
    "# Run Granger causality test with up to 3 lags\n",
    "grangercausalitytests(data, maxlag=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49447b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data: ensure it's sorted and aligned\n",
    "df = filtered_panel[['removals', 'value']].dropna().copy()\n",
    "df = df.astype(float)\n",
    "\n",
    "# Run Granger causality test: does 'removals' Granger-cause 'value'?\n",
    "# maxlag = 3 for testing 1, 2, 3 lags\n",
    "reverse_granger_results = grangercausalitytests(df[['value', 'removals']], maxlag=3, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d23f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run OLS regression with fixed effects for LA and year\n",
    "model = smf.ols(\n",
    "    formula='removals ~ value + C(local_authority) + C(financial_year) + C(size_category)+value:C(size_category)',\n",
    "    data=filtered_panel\n",
    ").fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(\n",
    "    formula=(\n",
    "        'removals ~ '\n",
    "        'value + value_lag1 + value_lag2 + value_lag3 + '\n",
    "        'C(local_authority) + C(financial_year) + C(size_category) + '\n",
    "        'value:C(size_category) + '\n",
    "        'value_lag1:C(size_category) + '\n",
    "        'value_lag2:C(size_category) + '\n",
    "        'value_lag3:C(size_category)'\n",
    "    ),\n",
    "    data=filtered_panel\n",
    ").fit()\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fb807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_panel_1 = panel_complete[\n",
    "    panel_complete['category'].str.startswith('total') &\n",
    "    ~panel_complete['category'].str.contains(', of which')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb59aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_complete['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7179b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_panel_2 = panel_complete[\n",
    "    panel_complete['category'].isin([\n",
    "        'total housing',\n",
    "        'total environmental & regulatory services',\n",
    "        'total culture & related services',\n",
    "        'tourism'\n",
    "    ])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264fc7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(\n",
    "    formula=(\n",
    "        'removals ~ value + C(local_authority) + C(financial_year) + C(category) + C(size_category) + value:C(size_category) + value:C(category)'\n",
    "    ),\n",
    "    data=filtered_panel_1\n",
    ").fit()\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1626576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, only on 'total housing'\n",
    "housing_panel = panel_complete[panel_complete['category'] == 'total culture & related services']\n",
    "model = smf.ols('removals ~ value + C(local_authority) + C(financial_year) + C(size_category)', data=housing_panel).fit()\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407692d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run OLS regression with fixed effects for LA and year\n",
    "model = smf.ols(\n",
    "    formula='value ~ removals + C(local_authority) + C(financial_year) + C(category) + C(size_category)+removals:C(size_category) + removals:C(category)',\n",
    "    data=filtered_panel_2\n",
    ").fit()\n",
    "\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
