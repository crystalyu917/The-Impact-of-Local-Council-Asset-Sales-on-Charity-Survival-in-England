{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f43fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import itertools\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e88ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPOSAL_FILEPATH = \"../../data/raw/council_disposal_receipts.xlsx\"\n",
    "\n",
    "YEAR_SKIP_MAPPING = {\n",
    "    \"2014\": 0,\n",
    "    \"2015\": 0,\n",
    "    \"2016\": 2,\n",
    "    \"2017\": 3,\n",
    "    \"2018\": 3,\n",
    "    \"2019\": 4,\n",
    "    \"2020\": 4,\n",
    "    \"2021\": 4,\n",
    "    \"2022\": 4,\n",
    "    \"2023\": 4,\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "for year, skip_row in YEAR_SKIP_MAPPING.items():\n",
    "    df = pd.read_excel(DISPOSAL_FILEPATH, skiprows=skip_row, sheet_name=year)\n",
    "    dfs.append(df)\n",
    "\n",
    "dataset = pd.read_csv(r'../../data/processed/charity_main_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaf1927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sheet(df, rename_from_col=3, header_row=1):\n",
    "    \"\"\"\n",
    "    Cleans a DataFrame read from Excel:\n",
    "    - Replaces values in `header_row` with column names starting from `rename_from_col`\n",
    "    - Keeps all columns up to `rename_from_col`\n",
    "    - Keeps only renamed columns after `rename_from_col`\n",
    "    - Sets row `header_row` as the header and drops all rows above it\n",
    "    \"\"\"\n",
    "    # Step 1: Identify renamed columns\n",
    "    renamed_mask = [not str(col).startswith(\"Unnamed:\") for col in df.columns]\n",
    "\n",
    "    # Step 2: Determine which columns to keep\n",
    "    cols_to_keep = list(range(rename_from_col + 1))  # Always keep up to and including `rename_from_col`\n",
    "    for i in range(rename_from_col + 1, len(df.columns)):\n",
    "        if renamed_mask[i]:\n",
    "            cols_to_keep.append(i)\n",
    "\n",
    "    # Step 3: Overwrite header_row with column names for renamed columns\n",
    "    for i in range(rename_from_col, len(df.columns)):\n",
    "        if renamed_mask[i]:\n",
    "            df.iloc[header_row, i] = df.columns[i]\n",
    "\n",
    "    # Step 4: Set row `header_row` as header and drop rows above\n",
    "    df.columns = df.iloc[header_row]\n",
    "    df = df.drop(index=list(range(header_row + 1))).reset_index(drop=True)\n",
    "\n",
    "    # Step 5: Keep only selected columns\n",
    "    df = df.iloc[:, cols_to_keep]\n",
    "\n",
    "    return df\n",
    "\n",
    "def rename_and_filter_disposal(df, start_col=5, keyword=\": Disposal of tangible fixed assets\"):\n",
    "    \"\"\"\n",
    "    Keeps:\n",
    "    - All columns before `start_col`\n",
    "    - Columns from `start_col` onward that contain `keyword`\n",
    "    \n",
    "    Renames matching columns by removing the keyword from their name.\n",
    "    \"\"\"\n",
    "    cols_to_keep = list(range(start_col))  # Keep early columns as-is\n",
    "\n",
    "    new_columns = df.columns.tolist()  # Copy of column names\n",
    "\n",
    "    for i in range(start_col, len(df.columns)):\n",
    "        col = str(df.columns[i])\n",
    "        if keyword in col:\n",
    "            # Rename column by removing the keyword\n",
    "            new_columns[i] = col.replace(keyword, \"\")\n",
    "            cols_to_keep.append(i)\n",
    "\n",
    "    # Apply renaming\n",
    "    df.columns = new_columns\n",
    "\n",
    "    # Keep only selected columns\n",
    "    return df.iloc[:, cols_to_keep]\n",
    "\n",
    "dfs[0] = clean_sheet(dfs[0], rename_from_col=3, header_row=1)\n",
    "dfs[1] = clean_sheet(dfs[1], rename_from_col=4, header_row=1)\n",
    "dfs[2] = clean_sheet(dfs[2], rename_from_col=4, header_row=2)\n",
    "dfs[3] = clean_sheet(dfs[3], rename_from_col=5, header_row=0)\n",
    "dfs[4] = clean_sheet(dfs[4], rename_from_col=5, header_row=0)\n",
    "\n",
    "dfs[5] = rename_and_filter_disposal(dfs[5], start_col=5)\n",
    "dfs[6] = rename_and_filter_disposal(dfs[6], start_col=5)\n",
    "dfs[7] = rename_and_filter_disposal(dfs[7], start_col=5)\n",
    "dfs[8] = rename_and_filter_disposal(dfs[8], start_col=5)\n",
    "dfs[9] = rename_and_filter_disposal(dfs[9], start_col=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73226bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    df = dfs[i]\n",
    "\n",
    "    # Step 1: Standardise column names first\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    if 'la name' in df.columns:\n",
    "        df = df.rename(columns={'la name': 'local_authority'})\n",
    "    \n",
    "    if 'local_authority' in df.columns:\n",
    "        df['local_authority'] = df['local_authority'].str.replace(r'(?i)\\(ua\\)|\\bua\\b', '', regex=True).str.strip()\n",
    "\n",
    "    # Step 2: Drop bad values in first column\n",
    "    first_col = df.columns[0]\n",
    "    df = df[~df[first_col].isin([pd.NA, None, '[z]', 'la_lgf_code'])]\n",
    "    df = df.dropna(subset=[first_col])\n",
    "\n",
    "    # Step 3: Drop missing ONS codes\n",
    "    if i == 0 and 'ecode' in df.columns:\n",
    "        df = df.dropna(subset=['ecode'])\n",
    "    elif i != 0 and 'ons code' in df.columns:\n",
    "        df = df.dropna(subset=['ons code'])\n",
    "\n",
    "    # Step 4: Drop unwanted council classes\n",
    "    if 'class' in df.columns:\n",
    "        df = df[~df['class'].isin(['O','SC'])]\n",
    "\n",
    "    # Save cleaned frame back\n",
    "    dfs[i] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7184a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_columns = {\n",
    "    \"community safety\": [\"community safety\", \"community safety (cctv)\"],\n",
    "    'agricultural & fisheries services':['agriculture & fisheries'],\n",
    "    'all services total':['total all services','all services'],\n",
    "    'early years & primary schools':['pre-primary & primary education'],\n",
    "    'parking':['parking of vehicles'],\n",
    "    'ports & piers':['local authority ports & piers'],\n",
    "    'special schools & alternative provision':['special education'],\n",
    "    'secondary schools':['secondary education'],\n",
    "    'total industrial & commercial trading':['industrial & commercial trading'],\n",
    "    'total environmental & regulatory services':['total environmental services','regulatory services (environmental health)'],\n",
    "    'tolled roads, bridges, tunnels,ferries & public transport companies':['tolled road bridges, tunnels, ferries, public transport companies','tolled road bridges, tunnels, ferries & public transport companies'],\n",
    "    'public transport (bus)':['public passenger transport - bus'],\n",
    "    'public transport (rail & other)':['public passenger transport - rail & other'],\n",
    "    'total housing':['housing'],\n",
    "    'total police':['police'],\n",
    "    'total social care':['social services','social care'],\n",
    "    'total public health':['public health'],\n",
    "    'roads, street lighting & road safety':['roads, street lights & safety'],\n",
    "    'total fire & rescue services':['fire & rescue services'],\n",
    "    'total central services':['central services (including court services)'],\n",
    "    'street cleaning (not chargeable to highways)':['street cleaning not chargeable to highways'],\n",
    "    'total planning & development':['total planning & development services','planning & development services'],\n",
    "    'total trading services':['total trading','trading'],\n",
    "    'total education':['education'],\n",
    "    'total highways & transport':['total transport','highways & transport'],\n",
    "    'total culture & related services':['culture & heritage']\n",
    "\n",
    "    #'commercial housing',\n",
    "    #'energy generation & supply',\n",
    "    #'finance & insurance activity',\n",
    "    #'hospitality & catering',\n",
    "    #'lgf code',\n",
    "    #'ons code',\n",
    "    #'other commercial activity',\n",
    "    #'other real estate activities',\n",
    "    #'post-16 provision & other education',\n",
    "    #'subclass',\n",
    "    #'total digital infrastructure',\n",
    "    #'water supply, sewerage & remediation'\n",
    "}\n",
    "\n",
    "# Standardise and merge alias columns across all dfs\n",
    "for idx in range(len(dfs)):\n",
    "    df = dfs[idx].copy()\n",
    "    df.columns = [\n",
    "        str(col).strip().lower()\n",
    "        .replace(\" and \", \" & \")\n",
    "        for col in df.columns\n",
    "    ]\n",
    "\n",
    "    dfs[idx] = df  # Save the cleaned version\n",
    "\n",
    "    for standard_col, aliases in merge_columns.items():\n",
    "        # Find which alias columns are present\n",
    "        present_cols = [col for col in aliases if col in df.columns]\n",
    "        if not present_cols:\n",
    "            continue  # Nothing to merge for this category\n",
    "\n",
    "        # Sum across all present columns\n",
    "        df[standard_col] = df[present_cols].sum(axis=1, skipna=True)\n",
    "\n",
    "        # Drop duplicates, except the one we’re keeping\n",
    "        cols_to_drop = [col for col in present_cols if col != standard_col]\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    dfs[idx] = df  # Save the cleaned DataFrame back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28bd720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined mapping: new_name → list of old names\n",
    "unified_map = {\n",
    "    'buckinghamshire': ['aylesbury vale', 'chiltern', 'south bucks', 'wycombe','south buckinghamshire'],\n",
    "    'dorset': ['weymouth and portland', 'west dorset', 'north dorset', 'east dorset', 'purbeck', 'christchurch'],\n",
    "    'somerset': ['taunton deane', 'west somerset', 'mendip', 'sedgemoor', 'south somerset', 'somerset council','somerset west and taunton'],\n",
    "    'cumberland': ['allerdale', 'carlisle', 'copeland', 'cumberland council'],\n",
    "    'westmorland and furness': ['barrow in furness', 'barrow-in-furness', 'eden', 'south lakeland'],\n",
    "    'north yorkshire': ['craven', 'hambleton', 'harrogate', 'richmondshire', 'ryedale', 'scarborough', 'selby', 'north yorkshire council'],\n",
    "    'bournemouth christchurch and poole': ['bournemouth', 'christchurch', 'poole'],\n",
    "    'west suffolk': ['forest heath', 'st edmundsbury'],\n",
    "    'east suffolk': ['suffolk coastal', 'waveney'],\n",
    "    'bath and north east somerset': ['bath and ne somerset'],\n",
    "    'southend-on-sea': ['southend on sea'],\n",
    "    'leicester': ['leicester city'],\n",
    "    'medway': ['medway towns'],\n",
    "    'derby': ['derby city'],\n",
    "    'folkestone and hythe': ['shepway'],\n",
    "    'county durham': ['durham'],\n",
    "    \"king's lynn and west norfolk\": ['kings lynn and west norfolk'],\n",
    "    'north northamptonshire': ['wellingborough', 'east northamptonshire', 'kettering', 'corby'],\n",
    "    'west northamptonshire': ['northampton', 'south northamptonshire', 'daventry'],\n",
    "}\n",
    "# old_name → new_name (lowercase)\n",
    "flat_lookup = {\n",
    "    old.lower(): new.lower()\n",
    "    for new, olds in unified_map.items()\n",
    "    for old in olds\n",
    "}\n",
    "def clean_local_authority(name):\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    name = str(name).lower()\n",
    "    name = (\n",
    "        name.replace('&', 'and')\n",
    "        .replace('-', ' ')\n",
    "        .replace(',', '')\n",
    "        .replace('.', '')\n",
    "        .replace(' city of', '')\n",
    "        .replace(' county of', '')\n",
    "        .strip()\n",
    "    )\n",
    "    return flat_lookup.get(name, name)\n",
    "# Apply to disposal files\n",
    "for i in range(len(dfs)):\n",
    "    if 'local_authority' in dfs[i].columns:\n",
    "        dfs[i]['local_authority'] = dfs[i]['local_authority'].apply(clean_local_authority)\n",
    "\n",
    "# Apply to charity dataset\n",
    "if 'local_authority' in dataset.columns:\n",
    "    dataset['local_authority'] = dataset['local_authority'].apply(clean_local_authority)\n",
    "\n",
    "# List of known devolved nation council names to exclude\n",
    "non_england_keywords = [\n",
    "    'aberdeen', 'aberdeenshire', 'angus', 'antrim', 'ards', 'argyll', 'armagh', 'belfast', 'blaenau', 'bridgend',\n",
    "    'caerphilly', 'cardiff', 'carmarthenshire', 'causeway', 'ceredigion', 'conwy', 'denbighshire', 'derry',\n",
    "    'dumfries', 'dundee', 'east ayrshire', 'east dunbartonshire', 'east lothian', 'east renfrewshire', 'falkirk',\n",
    "    'fermanagh', 'fife', 'flintshire', 'glasgow', 'gwynedd', 'highland', 'inverclyde', 'isle of man',\n",
    "    'isle of anglesey', 'lisburn', 'merthyr', 'mid and east antrim', 'mid ulster', 'midlothian', 'monmouthshire',\n",
    "    'moray', 'na h eileanan siar', 'neath', 'newport', 'newry', 'north ayrshire', 'north lanarkshire', 'orkney',\n",
    "    'pembrokeshire', 'perth and kinross', 'powys', 'renfrewshire', 'rhondda', 'scottish borders', 'shetland',\n",
    "    'south ayrshire', 'south lanarkshire', 'stirling', 'swansea', 'torfaen', 'vale of glamorgan', 'west dunbartonshire',\n",
    "    'west lothian', 'wrexham','city of edinburgh','channel islands'\n",
    "]\n",
    "\n",
    "# Convert to lowercase and filter out rows containing these names\n",
    "dataset['local_authority_lower'] = dataset['local_authority'].str.lower()\n",
    "dataset = dataset[~dataset['local_authority_lower'].str.contains('|'.join(non_england_keywords), na=False)]\n",
    "\n",
    "# Drop the helper column\n",
    "dataset = dataset.drop(columns=['local_authority_lower'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb290438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In disposal (dfs[0–9]) but NOT in charity dataset:\n",
      "[]\n",
      "\n",
      "🔍 In charity dataset but NOT in disposal (dfs[0–9]):\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get unique local_authority names from dfs[0–9]\n",
    "all_disposal_names = set()\n",
    "for i in range(10):\n",
    "    if 'local_authority' in dfs[i].columns:\n",
    "        all_disposal_names.update(dfs[i]['local_authority'].dropna().unique())\n",
    "\n",
    "# Step 2: Get unique local_authority names from dataset\n",
    "if 'local_authority' in dataset.columns:\n",
    "    charity_names = set(dataset['local_authority'].dropna().unique())\n",
    "else:\n",
    "    raise KeyError(\"'local_authority' column not found in dataset\")\n",
    "\n",
    "# Step 3: Find mismatches\n",
    "in_disposal_not_in_charity = all_disposal_names - charity_names\n",
    "in_charity_not_in_disposal = charity_names - all_disposal_names\n",
    "\n",
    "# Step 4: Print results\n",
    "print(\"✅ In disposal (dfs[0–9]) but NOT in charity dataset:\")\n",
    "print(sorted(in_disposal_not_in_charity))\n",
    "\n",
    "print(\"\\n🔍 In charity dataset but NOT in disposal (dfs[0–9]):\")\n",
    "print(sorted(in_charity_not_in_disposal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66bfd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Assign financial year to each DataFrame in dfs\n",
    "years = list(range(2014, 2024))\n",
    "for i, year in enumerate(years):\n",
    "    if not dfs[i].empty:\n",
    "        dfs[i]['financial_year'] = year\n",
    "\n",
    "# Step 2: Melt each DataFrame into long format\n",
    "long_frames = []\n",
    "for i, df in enumerate(dfs):\n",
    "    if df.empty:\n",
    "        continue\n",
    "    long_df = df.melt(\n",
    "        id_vars=[\"local_authority\", \"financial_year\"],\n",
    "        value_vars=[\n",
    "            col for col in df.columns \n",
    "            if col not in [\"ecode\", 'lgf code', 'ons code', \"class\", \"subclass\", \n",
    "                           \"local_authority\", \"financial_year\", 'certification complete']\n",
    "        ],\n",
    "        var_name=\"category\",\n",
    "        value_name=\"value\"\n",
    "    )\n",
    "    long_frames.append(long_df)\n",
    "\n",
    "# Step 3: Combine all disposal long-format DataFrames\n",
    "disposal_long_df = pd.concat(long_frames, ignore_index=True)\n",
    "\n",
    "# Step 4: Filter for 'all services total' category only\n",
    "disposal_long_df = disposal_long_df[disposal_long_df['category'] == 'all services total']\n",
    "\n",
    "# Step 5: Aggregate any duplicate disposal entries\n",
    "disposal_long_df = disposal_long_df.groupby(['local_authority', 'financial_year', 'category']).agg({\n",
    "    'value': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Step 6: Count removals from charity dataset and aggregate duplicates\n",
    "removals = (\n",
    "    dataset\n",
    "    .groupby(['local_authority', 'removal_fy', 'size_category'])\n",
    "    .size()\n",
    "    .reset_index(name='removals')\n",
    "    .rename(columns={'removal_fy': 'financial_year'})\n",
    ")\n",
    "\n",
    "# Step 7: Merge disposal data with charity removals\n",
    "panel = pd.merge(\n",
    "    disposal_long_df,\n",
    "    removals,\n",
    "    on=[\"financial_year\", \"local_authority\"],\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "# Step 8: Replace NaNs in 'removals' with 0 and filter years\n",
    "panel['removals'] = panel['removals'].fillna(0).astype(int)\n",
    "panel = panel[(panel['financial_year'] >= 2015) & (panel['financial_year'] <= 2023)]\n",
    "panel['value'] = pd.to_numeric(panel['value'], errors='coerce')/1000 # original unit - thousand - to million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "126ddb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique values for each dimension (excluding NaN values)\n",
    "unique_years = panel['financial_year'].unique()\n",
    "unique_authorities = panel['local_authority'].unique()\n",
    "unique_size_categories = panel['size_category'].dropna().unique()\n",
    "\n",
    "# Create all possible combinations\n",
    "all_combinations = list(itertools.product(unique_years, unique_authorities, unique_size_categories))\n",
    "\n",
    "# Convert to DataFrame\n",
    "complete_index = pd.DataFrame(all_combinations, \n",
    "                             columns=['financial_year', 'local_authority', 'size_category'])\n",
    "\n",
    "# Merge with original panel data\n",
    "complete_panel = pd.merge(\n",
    "    complete_index,\n",
    "    panel,\n",
    "    on=['financial_year', 'local_authority', 'size_category'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values appropriately\n",
    "# For 'removals' column, fill with 0 (no removals)\n",
    "complete_panel['removals'] = complete_panel['removals'].fillna(0).astype(int)\n",
    "# Fill missing 'value' and 'category' by forward filling within each local_authority-financial_year group\n",
    "complete_panel['value'] = complete_panel.groupby(['local_authority', 'financial_year'])['value'].transform('first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd5a6a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure proper sorting\n",
    "filtered_panel = complete_panel.sort_values(by=['local_authority', 'size_category', 'financial_year'])\n",
    "filtered_panel = filtered_panel.drop('category', axis=1)\n",
    "filtered_panel = filtered_panel.dropna(subset=['value'])\n",
    "\n",
    "# Create lagged value columns\n",
    "filtered_panel['value_lag1'] = (\n",
    "    filtered_panel\n",
    "    .groupby(['local_authority', 'size_category'])['value']\n",
    "    .shift(1)\n",
    ")\n",
    "\n",
    "filtered_panel['value_lag2'] = (\n",
    "    filtered_panel\n",
    "    .groupby(['local_authority', 'size_category'])['value']\n",
    "    .shift(2)\n",
    ")\n",
    "\n",
    "filtered_panel['value_lag3'] = (\n",
    "    filtered_panel\n",
    "    .groupby(['local_authority', 'size_category'])['value']\n",
    "    .shift(3)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f07955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_panel.to_csv('filtered_panel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df7cb251",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_authorities = (\n",
    "    filtered_panel['local_authority']\n",
    "    .dropna()\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .to_frame(name='local_authority')\n",
    ")\n",
    "\n",
    "unique_authorities.to_csv('list_of_local_authority.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ee2e54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\UCL_Dissertation\\The-change-in-the-UK-charity-landscape\\src\\cleaning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede2792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF test on 'removals'\n",
    "result_removals = adfuller(filtered_panel['removals'].dropna())\n",
    "print(\"ADF Test for 'removals':\")\n",
    "print(f\"ADF Statistic: {result_removals[0]}\")\n",
    "print(f\"p-value: {result_removals[1]}\")\n",
    "print(f\"Critical Values: {result_removals[4]}\\n\")\n",
    "\n",
    "# ADF test on 'value'\n",
    "result_value = adfuller(filtered_panel['value'].dropna())\n",
    "print(\"ADF Test for 'value':\")\n",
    "print(f\"ADF Statistic: {result_value[0]}\")\n",
    "print(f\"p-value: {result_value[1]}\")\n",
    "print(f\"Critical Values: {result_value[4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4291d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Engle-Granger cointegration test\n",
    "coint_stat, p_value, crit_values = coint(filtered_panel['removals'], filtered_panel['value'])\n",
    "\n",
    "print(f\"Engle-Granger Cointegration Test\")\n",
    "print(f\"Test Statistic: {coint_stat}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "print(f\"Critical Values: {crit_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999e5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data as a two-column array: [removals, value]\n",
    "data = filtered_panel[['removals', 'value']].dropna()\n",
    "\n",
    "# Run Granger causality test with up to 3 lags\n",
    "grangercausalitytests(data, maxlag=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49447b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data: ensure it's sorted and aligned\n",
    "df = filtered_panel[['removals', 'value']].dropna().copy()\n",
    "df = df.astype(float)\n",
    "\n",
    "# Run Granger causality test: does 'removals' Granger-cause 'value'?\n",
    "# maxlag = 3 for testing 1, 2, 3 lags\n",
    "reverse_granger_results = grangercausalitytests(df[['value', 'removals']], maxlag=3, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d23f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run OLS regression with fixed effects for LA and year\n",
    "model = smf.ols(\n",
    "    formula='removals ~ value + C(local_authority) + C(financial_year) + C(size_category)+value:C(size_category)',\n",
    "    data=filtered_panel\n",
    ").fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(\n",
    "    formula=(\n",
    "        'removals ~ '\n",
    "        'value + value_lag1 + value_lag2 + value_lag3 + '\n",
    "        'C(local_authority) + C(financial_year) + C(size_category) + '\n",
    "        'value:C(size_category) + '\n",
    "        'value_lag1:C(size_category) + '\n",
    "        'value_lag2:C(size_category) + '\n",
    "        'value_lag3:C(size_category)'\n",
    "    ),\n",
    "    data=filtered_panel\n",
    ").fit()\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_panel[filtered_panel['local_authority']=='north yorkshire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_removals = filtered_panel.sort_values('removals', ascending=False).head(30)\n",
    "print(top10_removals[['local_authority', 'financial_year', 'removals', 'value', 'size_category']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0debbee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_panel_1 = filtered_panel.copy()\n",
    "filtered_panel_1['value_thousands'] = filtered_panel_1['value'] * 1000\n",
    "\n",
    "sns.lmplot(\n",
    "    data=filtered_panel_1,\n",
    "    x='value_thousands', y='removals',\n",
    "    hue='size_category',  # group by charity size\n",
    "    lowess=True,\n",
    "    scatter_kws={'alpha':0.3},\n",
    "    line_kws={'linewidth':2},\n",
    "    height=5, aspect=1.2\n",
    ")\n",
    "\n",
    "plt.xlabel('Capital Receipts (£1,000)')\n",
    "plt.ylabel('Charity Removals')\n",
    "plt.title('Removals vs Capital Receipts by Charity Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ffafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\n",
    "    data=filtered_panel_1,\n",
    "    x='value_thousands', y='removals',\n",
    "    hue='size_category',\n",
    "    lowess=True,\n",
    "    scatter_kws={'alpha':0.3},\n",
    "    line_kws={'linewidth':2},\n",
    "    height=5, aspect=1.2\n",
    ")\n",
    "\n",
    "plt.xlabel('Capital Receipts (£1,000)')\n",
    "plt.ylabel('Charity Removals')\n",
    "plt.title('Removals vs Capital Receipts by Charity Size')\n",
    "plt.xlim(0, 100000)  # limit to £100 million\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
